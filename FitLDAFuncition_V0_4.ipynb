{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB351jg2g64p",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\usuario\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Collecting gensim\n  Using cached https://files.pythonhosted.org/packages/3a/bc/1415be59292a23ff123298b4b46ec4be80b3bfe72c8d188b58ab2653dee4/gensim-3.8.0.tar.gz\nRequirement already satisfied: numpy>=1.11.3 in c:\\notebooks\\venv\\lib\\site-packages (from gensim) (1.17.2)\nRequirement already satisfied: scipy>=0.18.1 in c:\\notebooks\\venv\\lib\\site-packages (from gensim) (1.3.1)\nRequirement already satisfied: six>=1.5.0 in c:\\notebooks\\venv\\lib\\site-packages (from gensim) (1.12.0)\nCollecting smart_open>=1.7.0 (from gensim)\nCollecting boto3 (from smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/46/71/d7fb187009752b589325b55d06b046f87551985b6a46dd70518b59036d5c/boto3-1.9.234-py2.py3-none-any.whl (128kB)\nCollecting requests (from smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\nCollecting boto>=2.32 (from smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\nCollecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart_open>=1.7.0->gensim)\n  Using cached https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl\nCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart_open>=1.7.0->gensim)\n  Using cached https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\nCollecting botocore<1.13.0,>=1.12.234 (from boto3->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/52/6f/7c5270b26967e7e586de26ff64b71192c4444896e229ee011428b62483d9/botocore-1.12.234-py2.py3-none-any.whl (5.7MB)\nCollecting idna<2.9,>=2.5 (from requests->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\nCollecting chardet<3.1.0,>=3.0.2 (from requests->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/81/b7/cef47224900ca67078ed6e2db51342796007433ad38329558f56a15255f5/urllib3-1.25.5-py2.py3-none-any.whl (125kB)\nCollecting certifi>=2017.4.17 (from requests->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\notebooks\\venv\\lib\\site-packages (from botocore<1.13.0,>=1.12.234->boto3->smart_open>=1.7.0->gensim) (2.8.0)\nCollecting docutils<0.16,>=0.10 (from botocore<1.13.0,>=1.12.234->boto3->smart_open>=1.7.0->gensim)\n  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\nInstalling collected packages: jmespath, docutils, urllib3, botocore, s3transfer, boto3, idna, chardet, certifi, requests, boto, smart-open, gensim\n  Running setup.py install for gensim: started\n    Running setup.py install for gensim: finished with status 'done'\nSuccessfully installed boto-2.49.0 boto3-1.9.234 botocore-1.12.234 certifi-2019.9.11 chardet-3.0.4 docutils-0.15.2 gensim-3.8.0 idna-2.8 jmespath-0.9.4 requests-2.22.0 s3transfer-0.2.1 smart-open-1.8.4 urllib3-1.25.5\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#!pip install -U spacy\n",
    "#!python -m spacy download es_core_news_sm\n",
    "##!python -m spacy validate\n",
    "##!python -m spacy link es_core_news_sm en --force\n",
    "nltk.download('stopwords')\n",
    "!pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3052,
     "status": "error",
     "timestamp": 1568393190927,
     "user": {
      "displayName": "IVAN DARIO GOMEZ RINCON",
      "photoUrl": "",
      "userId": "08988108265814176031"
     },
     "user_tz": 300
    },
    "id": "ph1kMj7diQNu",
    "outputId": "8ee3ba0f-adff-4b00-b475-20163d1706ca",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a82e8b6a7743>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m##nlp = sc.load('es_core_news_sm')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mes_core_news_sm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mes_core_news_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'es_core_news_sm'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'es_core_news_sm'",
     "output_type": "error"
    }
   ],
   "source": [
    "#librerias generales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "import statsmodels.tsa.stattools as stt\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "from dateutil.relativedelta import relativedelta\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#librerias para los modelos de series de tiempo\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR, DynamicVAR\n",
    "\n",
    "#Libresrias de pre-procesamiento de texto\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "spanish_stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "spanish_stopwords.extend(['Junta','Directiva','semestre','bien','entornar','aunque','respictavemente', 'probable',\n",
    "                         'menos', 'me', '3', 'millón', 'punto', 'si', 'futuro', 'mostrar', 'septiembre', 'enero','1', 'b'\n",
    "                         'febrero', 'marzo', 'abril', 'mayo', 'junio','julio','agosto', 'octubre', 'noviembre','diciembre'\n",
    "                          \n",
    "                         'anterior', 'frente', 'largar', 'tiempo', 'partir', 'cercar', 'señalar', 'mes', 'registrar'\n",
    "                         'mesar', 'atrás', 'estimar', 'proyectar','miembro', 'europa', 'pb', 'finalmente', 'observar'\n",
    "                         'registrar', 'segundar', 'sobrar', 'rango', 'ingresar' ])\n",
    "import scipy as sc\n",
    "##nlp = sc.load('es_core_news_sm')\n",
    "import es_core_news_sm\n",
    "nlp = es_core_news_sm.load()\n",
    "\n",
    "#Librerias para modelo LDA\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvbWI1lWg64u"
   },
   "source": [
    "## Cargar series de tiempo en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZnQfDTNg64v",
    "outputId": "a04f1004-aed1-4c69-977e-453205d0a5b9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\usuario\\Google Drive\\Varios Ivan\\1 Maestria\\Universidad de los Andes\\ProyectoGrado\\Desarrollo\\dataIndicadores.xlsx', sheet_name = 'Var_Men', encoding='utf-8')\n",
    "\n",
    "data.Fecha = pd.to_datetime(data.Fecha)\n",
    "data = data.set_index('Fecha')\n",
    "data = data.dropna()\n",
    "\n",
    "cycle, trend = sm.tsa.filters.hpfilter(data.IPC, 1600)\n",
    "data['IPC'] = cycle\n",
    "\n",
    "cycle, trend = sm.tsa.filters.hpfilter(data.LIBOR, 1600)\n",
    "data['LIBOR'] = cycle\n",
    "\n",
    "for i in data.columns.tolist():\n",
    "    result = stt.adfuller(data[str(i)])\n",
    "    pvalue = float(result[1])\n",
    "    isStationary = pvalue <= 0.05\n",
    "    print(i)\n",
    "    print(\"P-VALUE TASA: \", pvalue, isStationary)\n",
    "    print('*'*40)\n",
    "    print(' ')\n",
    "\n",
    "data_series = data\n",
    "data_series.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ed9h3R_1iQNx"
   },
   "source": [
    "## Cargar documentos en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsGRGyn6iQNy",
    "outputId": "d0e37e75-021b-427a-e925-a155399dd668",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Para listar los archivos de directorios\n",
    "mypath = \"C:/Users/usuario/Google Drive/Varios Ivan/1 Maestria/Universidad de los Andes/ProyectoGrado/Desarrollo/Comunicados/\"\n",
    "files = listdir(mypath)\n",
    "\n",
    "#Cargar textos en un vector\n",
    "docVector = []\n",
    "for i in range(len(files)):\n",
    "    file = open(mypath + files[i],\"r\", encoding=\"utf8\") \n",
    "    #print(files[i])\n",
    "    docVector.append({'text': file.read(), 'Date': files[i][:7]})\n",
    "\n",
    "#cargar textos en un dataframe\n",
    "docDataF = pd.DataFrame(docVector)\n",
    "docDataF.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJAkGCETiQN3"
   },
   "source": [
    "### Función de limpieza y preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNCUteEUiQN4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text, remove_stop_words=True):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    document = text\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    #Removing punctuation\n",
    "    document = re.sub(r'[^\\w\\s]', '', document)\n",
    "    \n",
    "    #Removing numbers\n",
    "    document = ''.join(i for i in document if not i.isdigit())\n",
    "\n",
    "    #LowerCase    \n",
    "    document = document.lower()\n",
    "    \n",
    "    #Split document word a word\n",
    "    #words_document = document.split()\n",
    "    \n",
    "    words_document = []    \n",
    "    doc = nlp(document)\n",
    "    for token in doc: words_document.append(token.lemma_)\n",
    "\n",
    "    #Remove stop words\n",
    "    if remove_stop_words:\n",
    "        words_document = [word for word in words_document if word not in spanish_stopwords]\n",
    "    \n",
    "    #Lemmatisation\n",
    "    #words_document = [wordnet_lemmatizer.lemmatize(word) for word in words_document]\n",
    "    #words_document = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words_document]\n",
    "    \n",
    "    #stimming\n",
    "    #words_document = [stemmer.stem(word) for word in words_document] \n",
    "           \n",
    "    #return ' '.join(words_document)\n",
    "    return words_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwdlYi47iQN5"
   },
   "source": [
    "### Función de vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZwgENqniQN6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def vectorizarTextos (documents_):\n",
    "    dictionary = gensim.corpora.Dictionary(documents_)\n",
    "    #Filter out tokens that appear in too much documents\n",
    "    #less than 15 documents (absolute number) or\n",
    "    #more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "    #after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.9, keep_n=10000)\n",
    "    #print(dictionary.token2id)\n",
    "    #len(dictionary)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in documents_]\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    \n",
    "    return dictionary, corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVO_zORtg645"
   },
   "source": [
    "#### ¨prueba paralelizar\n",
    "def fitLDA_ModelParallel(corpus_tfidf_ \n",
    "                         ,dictionary_ \n",
    "                         ,alpha_\n",
    "                         ,rangEta = range(1, 5)\n",
    "                         ,rangTopics = range(4, 10, 1)\n",
    "                         ,resNumTopPala = 20):\n",
    "    \n",
    "    resultMatrixParalell_ = []\n",
    "    for eta_ in rangEta:\n",
    "        for num_topics_ in rangTopics:\n",
    "            print(alpha_, eta_, num_topics_ )\n",
    "            #Ajustar el modelo con la configuración que corresponda en el for\n",
    "            lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf_, \n",
    "                                                         id2word = dictionary_, \n",
    "                                                         alpha = alpha_, \n",
    "                                                         eta = eta_, \n",
    "                                                         num_topics = num_topics_, \n",
    "                                                         workers=3,\n",
    "                                                         random_state=42)\n",
    "\n",
    "            #Obtener la distribución de los documentos en los tópicos\n",
    "            topics = lda_model_tfidf.get_document_topics(corpus_tfidf_, \n",
    "                                                         minimum_probability=None, \n",
    "                                                         minimum_phi_value=None, \n",
    "                                                         per_word_topics=False)\n",
    "\n",
    "            #Guarda el top n paralabra en el resultado, según parámetro resNumTopPala\n",
    "            top_Palabras = []\n",
    "            for i in range(0,num_topics_):\n",
    "                top_Palabras.extend( np.append(np.full((resNumTopPala,1), i+1), lda_model_tfidf.show_topic(i, topn=resNumTopPala), axis=1 ) )\n",
    "\n",
    "            resultMatrixParalell_.append([alpha_, eta_, num_topics_, topics, top_Palabras])\n",
    "    \n",
    "    \n",
    "    return resultMatrixParalell_\n",
    "\n",
    "\n",
    "\n",
    "def fitLDA_Model(corpus_tfidf_ \n",
    "                 ,dictionary_ \n",
    "                 ,rangAlpha = np.arange(0.1, 1, 0.1)\n",
    "                 ,rangEta = range(1, 5)\n",
    "                 ,rangTopics = range(4, 10, 1)\n",
    "                 ,resNumTopPala = 20):\n",
    "    \n",
    "    resultMatrix_ = Parallel(n_jobs=-1, verbose=1)(delayed(fitLDA_ModelParallel)(corpus_tfidf_, dictionary_, alpha) for alpha in rangAlpha)\n",
    "    return resultMatrix_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4frXNwPWiQN7"
   },
   "source": [
    "### Función para hallar los parámetros del modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xajevf2iiQN8",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Función lineal - arriba está la función paralelizada\n",
    "# def fitLDA_Model(corpus_tfidf_ \n",
    "#                  ,dictionary_ \n",
    "#                  ,rangAlpha = np.arange(0.4, 1.6, 0.2)\n",
    "#                  ,rangEta = np.arange(0.4, 1.6, 0.2)\n",
    "#                  ,rangTopics = range(4, 10, 1)\n",
    "#                  ,resNumTopPala = 30):\n",
    "    \n",
    "def fitLDA_Model(corpus_tfidf_ \n",
    "                 ,dictionary_ \n",
    "                 ,rangAlpha = np.arange(0.2, 0.6, 0.05)\n",
    "                 ,rangEta = np.arange(0.4, 3, 0.4)\n",
    "                 ,rangTopics = range(3, 6, 1)\n",
    "                 ,resNumTopPala = 30):\n",
    "\n",
    "    rangAlpha = rangAlpha.round(decimals=2)\n",
    "    rangEta = rangEta.round(decimals=2)\n",
    "    \n",
    "    resultMatrix_ = []\n",
    "    for alpha_ in rangAlpha:\n",
    "        for eta_ in rangEta:\n",
    "            for num_topics_ in rangTopics:\n",
    "                print(alpha_, eta_, num_topics_ )\n",
    "                #Ajustar el modelo con la configuración que corresponda en el for\n",
    "                lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf_, \n",
    "                                                             id2word = dictionary_, \n",
    "                                                             alpha = alpha_, \n",
    "                                                             eta = eta_, \n",
    "                                                             num_topics = num_topics_, \n",
    "                                                             workers=7,\n",
    "                                                             random_state=42)\n",
    "                \n",
    "                #Obtener la distribución de los documentos en los tópicos\n",
    "                topics = lda_model_tfidf.get_document_topics(corpus_tfidf_, \n",
    "                                                             minimum_probability=None, \n",
    "                                                             minimum_phi_value=None, \n",
    "                                                             per_word_topics=False)\n",
    "                \n",
    "                #Guarda el top n paralabra en el resultado, según parámetro resNumTopPala\n",
    "                top_Palabras = []\n",
    "                for i in range(0,num_topics_):\n",
    "                    top_Palabras.extend( np.append(np.full((resNumTopPala,1), i+1), lda_model_tfidf.show_topic(i, topn=resNumTopPala), axis=1 ) )\n",
    "                \n",
    "                resultMatrix_.append([alpha_, eta_, num_topics_, topics, top_Palabras])\n",
    "                \n",
    "    return resultMatrix_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeEZZ2M4g649"
   },
   "source": [
    "## Función lineal - arriba está la función paralelizada\n",
    "def fitLDA_Model(corpus_tfidf_ \n",
    "                 ,dictionary_ \n",
    "                 ,rangAlpha = np.arange(0.1, 1, 0.1)\n",
    "                 ,rangEta = range(1, 5)\n",
    "                 ,rangTopics = range(4, 10, 1)\n",
    "                 ,resNumTopPala = 20):\n",
    "    \n",
    "   \n",
    "    resultMatrix_ = []\n",
    "    #for alpha_ in rangAlpha:\n",
    "    #    for eta_ in rangEta:\n",
    "    for num_topics_ in rangTopics:\n",
    "        print( num_topics_ )\n",
    "        #Ajustar el modelo con la configuración que corresponda en el for\n",
    "        lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf_, \n",
    "                                                     id2word = dictionary_, \n",
    "                                                     num_topics = num_topics_, \n",
    "                                                     workers=-1,\n",
    "                                                     random_state=42)\n",
    "\n",
    "        #Obtener la distribución de los documentos en los tópicos\n",
    "        topics = lda_model_tfidf.get_document_topics(corpus_tfidf_, \n",
    "                                                     minimum_probability=None, \n",
    "                                                     minimum_phi_value=None, \n",
    "                                                     per_word_topics=False)\n",
    "\n",
    "        #Guarda el top n paralabra en el resultado, según parámetro resNumTopPala\n",
    "        top_Palabras = []\n",
    "        for i in range(0,num_topics_):\n",
    "            top_Palabras.extend( np.append(np.full((resNumTopPala,1), i+1), lda_model_tfidf.show_topic(i, topn=resNumTopPala), axis=1 ) )\n",
    "\n",
    "        resultMatrix_.append([-1, -1, num_topics_, topics, top_Palabras])\n",
    "                \n",
    "    return resultMatrix_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o5yJF62oiQN-"
   },
   "source": [
    "### Función para calcular modelos VAR y LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zydk4uqkiQN_",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def fit_VARModel(docDataF_, indicadorSerie, data_series_, num_periodos_test):\n",
    "\n",
    "    resultSeries_ = []\n",
    "\n",
    "    #correr el ajuste del LDA mes a mes por el numero de periodos de test seleccionados\n",
    "    #en cada iteración se calcula un gridsearch con los parametros del LDA\n",
    "    for i in range (0, num_periodos_test):\n",
    "        \n",
    "        documents=[]\n",
    "        #1.1 Correr función \"preprocess\" enviando el texto mensualizado removiendo los periodos de test 1 a la vez \n",
    "        for texto in docDataF_[:-num_periodos_test+i].text:\n",
    "            documents.append(preprocess(texto))\n",
    "        print(\"num_mes_test:\", i, \"num_docs\", len(documents) )\n",
    "        \n",
    "        #1.2 Vectorizar los textos con TFIDF, la funcion retorna el diccionario y el texto vectorizado\n",
    "        dictionary, corpus_tfidf = vectorizarTextos (documents)\n",
    "\n",
    "        #1.3 Ajustar los modelos LDA con las posible combinaciones de alpha, eta y número de topicos\n",
    "        # Retorna la distribución de tópicos por cada documento de acuerdo con el modelo LDA ajustado con los diferentes parámetros\n",
    "        paramsTopics_ = fitLDA_Model(corpus_tfidf, dictionary)\n",
    "        \n",
    "        result_ = []\n",
    "        #Correr un modelo VAR para cada resultado de LDA de acuerdo con el gridsearch\n",
    "        for num_model in range(0,len(paramsTopics_)):\n",
    "            topics = paramsTopics_[num_model][3]\n",
    "            #Crear un Dataframe de topicos en los documentos como serie de tiempo\n",
    "            topics_df = pd.DataFrame(gensim.matutils.corpus2csc(topics).T.toarray())\n",
    "            \n",
    "            #Unir los topicos con el indicador en el mismo DataFrame\n",
    "            data = indicadorSerie[:-num_periodos_test+i].reset_index().iloc[:,1:].join(topics_df).join(data_series_[:-num_periodos_test+i])\n",
    "            #data.to_pickle(\"Data.pkl\") #Guardar resultado parcial\n",
    "            \n",
    "            #El numero 1 indica el numero de resagos\n",
    "            model = VAR(data)            \n",
    "            results = model.fit(1)\n",
    "\n",
    "            #proyectar test\n",
    "            proy_var = pd.DataFrame(results.forecast(data.values[-results.k_ar:], 1))\n",
    "            #print(\"num_model:\", num_model, \" proy:\", proy_var.values[:,0])\n",
    "            \n",
    "            result_.append([ proy_var.values[:,0], indicadorSerie.iloc[-num_periodos_test+i,0] ] )\n",
    "            \n",
    "        #print(result_)\n",
    "        \n",
    "        resultSeries_.append(np.append(result_, paramsTopics_, axis=1) )        \n",
    "    \n",
    "    return resultSeries_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8ts2xnLiQOB"
   },
   "source": [
    "# Flujo de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TD0aKB6bg65D",
    "outputId": "f3db50ad-66fc-4f3b-f080-b073088ff935",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Inicio = 2019-09-23 15:31:41.424481\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5376309ab298>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inicio =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0motras_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_series\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Fecha\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"TASA_POL_MON\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresultSeries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_VARModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocDataF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TASA_POL_MON\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0motras_series\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fin =\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_series' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'data_series' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "print(\"Inicio =\", datetime.now())\n",
    "otras_series = data_series.reset_index().drop([\"Fecha\", \"TASA_POL_MON\"], axis=1)\n",
    "resultSeries = fit_VARModel(docDataF, data_series[[\"TASA_POL_MON\"]],otras_series, 24)\n",
    "print(\"Fin =\", datetime.now())\n",
    "\n",
    "# Ultima ejecución\n",
    "# Inicio = 2019-09-17 23:05:17.495484\n",
    "# Fin = 2019-09-18 03:20:36.631524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTB4HoHNg65H",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Guardar resultado\n",
    "fileObject = open(\"Result_17Sep_lin.pkl\", 'wb')\n",
    "pickle.dump(resultSeries, fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7vICB8kg65I"
   },
   "source": [
    "# Carga y transforma resultados para reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RkwZxF-Dg65J",
    "outputId": "f9bd9053-cbaa-48ad-c762-da5f256bff63",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "## Cargar resultado guardado\n",
    "fileObject2 = open(\"Result_17Sep_lin.pkl\", 'rb')\n",
    "resultLoaded = pickle.load(fileObject2)\n",
    "fileObject2.close()\n",
    "# Leer un registro para validar\n",
    "resultLoaded[1][1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-LwVyiHg65L"
   },
   "source": [
    "## Transformar y crear arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7iMBErog65L",
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Nivel 1: Mes BackTest\n",
    "#Nivel 2: Modelo LDA\n",
    "#Nivel 3: Proyeccion, valor Original, Parámetros, tópicos y palabras\n",
    "#resultLoaded[0][0][:4]\n",
    "dataModel = []\n",
    "modelTopicos = []\n",
    "modelPalabras = []\n",
    "for nivel1 in range(0,len(resultLoaded)): \n",
    "    for nivel2 in range(0,len(resultLoaded[nivel1])): \n",
    "        \n",
    "        dataModel.append( np.append([nivel1],\n",
    "                           np.append([nivel2], \n",
    "                           resultLoaded[nivel1][nivel2][:5], axis=0), axis=0))\n",
    "        \n",
    "        lonTopicos = len(resultLoaded[nivel1][nivel2][5])\n",
    "        modelTopicos.extend(np.append(np.full((lonTopicos,1), nivel1),\n",
    "                            np.append(np.full((lonTopicos,1), nivel2),\n",
    "                            np.append(np.indices((lonTopicos,1))[0] ,\n",
    "                            gensim.matutils.corpus2csc(resultLoaded[nivel1][nivel2][5]).T.toarray(), axis=1), axis=1), axis=1))\n",
    "        \n",
    "        lonPalabra = len(resultLoaded[nivel1][nivel2][6])\n",
    "        modelPalabras.extend(np.append(np.full((lonPalabra,1), nivel1),\n",
    "                             np.append(np.full((lonPalabra,1), nivel2),\n",
    "                             resultLoaded[nivel1][nivel2][6], axis=1), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqjoEjcwg65N"
   },
   "source": [
    "## Crear dataframes y transformar formatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fp6kGCJag65N",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#import calendar\n",
    "\n",
    "#Crea Dataframe modelos\n",
    "dataModel_df = pd.DataFrame(dataModel, columns=['MesBack','ModeloLDA','Proyec','OrgValue','Alpha','Eta','Topics' ])\n",
    "fec_end = max(data_series.index)\n",
    "lenback = max(dataModel_df.MesBack)\n",
    "dataModel_df.MesBack = dataModel_df.MesBack.apply(lambda x: fec_end - relativedelta(months=lenback-x))\n",
    "\n",
    "#Crea Dataframe topicos\n",
    "modelTopicos_df = pd.DataFrame(modelTopicos)\n",
    "modelTopicos_df.rename(columns= {0: \"MesBack\", 1:\"Modelo\", 2:\"MesDocumento\", 3:'T1',4:'T2',5:'T3',6:'T4',7:'T5',8:'T6',9:'T7',10:'T8',11:'T9'}, inplace = True) \n",
    "modelTopicos_df.MesBack = modelTopicos_df.MesBack.apply(lambda x: fec_end - relativedelta(months=lenback-x))\n",
    "\n",
    "#Crea Dataframe palabras\n",
    "modelPalabras_df = pd.DataFrame(modelPalabras, columns=['MesBack','ModeloLDA','Topic','Palabra','Importancia'])\n",
    "modelPalabras_df.MesBack = modelPalabras_df.MesBack.apply(lambda x: fec_end - relativedelta(months=lenback-x.astype(int)))\n",
    "modelTopicos_df.Modelo = modelTopicos_df.Modelo.astype(int)\n",
    "fec_ini = min(data_series.index)\n",
    "modelTopicos_df.MesDocumento = modelTopicos_df.MesDocumento.apply(lambda x: fec_ini + relativedelta(months=x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFtjE1sog65P",
    "outputId": "c4de0079-ac06-456a-846f-a2f5853bc876",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataModel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMD-tbAyg65R",
    "outputId": "6a04bbeb-ee1b-4b11-f3de-72b135b5e315",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "modelTopicos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qKhN1czsg65T",
    "outputId": "3d778e13-a5c7-4f2e-86e9-d2aff1dacaa0",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "modelPalabras_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VJbaZ8B6g65W"
   },
   "source": [
    "## Exportar a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNS8qHKLg65Y",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataModel_df.to_csv(\"Modelos.csv\", index = False)\n",
    "modelTopicos_df.to_csv(\"Topicos.csv\", index = False)\n",
    "modelPalabras_df.to_csv(\"Palabras.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMbjAqzug65b",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy_rbrQvg65e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XNXDg8NVg65l",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HFhJpkIg65m",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyHGjlxHg65o"
   },
   "source": [
    "# Codigo de pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzi0FfsGg65p",
    "outputId": "3dc7b016-f2c3-456e-ab45-7ea805e57e01",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Ejemplo para:\n",
    "#Nivel 1: Primer mes de Back Test 0\n",
    "#Nivel 2: Modelo LDA 0\n",
    "#Nivel 3: Se imprime a continuación\n",
    "mes= 3\n",
    "model = 10\n",
    "print(\"Error\",resultLoaded[mes][model][0], \"Alpha: \",resultLoaded[mes][model][1], \" Eta:\", resultLoaded[mes][model][2], \" NumTopics:\", resultLoaded[mes][model][3])\n",
    "\n",
    "#Convertir el array top de palabras en un DataFrame\n",
    "palabrasModel = pd.DataFrame(resultLoaded[mes][model][5], columns=['Topic','Palabra','Importancia'])\n",
    "palabrasModel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcW4DuZkg65r",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "##########       TEST      ###########\n",
    "######################################\n",
    "\n",
    "documents=[]\n",
    "\n",
    "#1.1 Correr función \"preprocess\" enviand el texto de al cual se le va aplicar LDA, el resultado se guarda en el vector documents\n",
    "for texto in docDataF.text:    \n",
    "    documents.append(preprocess(texto))\n",
    "\n",
    "#1.2 Vectorizar los textos con TFIDF, la funcion retorna el diccionario y el texto vectorizado\n",
    "dictionary, corpus_tfidf = vectorizarTextos (documents)\n",
    "\n",
    "num_topics_=8\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             id2word = dictionary, \n",
    "                                             alpha = 0.5, \n",
    "                                             eta = 1, \n",
    "                                             num_topics=num_topics_, \n",
    "                                             workers=3, \n",
    "                                             random_state=42)\n",
    "                \n",
    "#Obtener la distribución de los documentos en los tópicos\n",
    "topics = lda_model_tfidf.get_document_topics(corpus_tfidf, \n",
    "                                             minimum_probability=None, \n",
    "                                             minimum_phi_value=None, \n",
    "                                             per_word_topics=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcgsclP3g65t",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Prueba Paralelo\n",
    "otras_series = data_series.reset_index().drop([\"Fecha\", \"TASA_POL_MON\"], axis=1)\n",
    "resultSeries = fit_VARModel(docDataF, data_series[[\"TASA_POL_MON\"]],otras_series, 2)\n",
    "\n",
    "### Función para probar todos los inidcadores \n",
    "topicsResult = []\n",
    "for column in data_series.columns.tolist():\n",
    "    print(column)\n",
    "    otras_series = data_series.reset_index().drop([\"Fecha\", column], axis=1)\n",
    "    resultSeries = fit_VARModel(docDataF, data_series[[column]],otras_series, 36)\n",
    "    topicsResult.append(resultSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3OnYlkAg65v",
    "outputId": "f8fa721f-2d82-4699-8eff-6aac45631e28",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(gensim.matutils.corpus2csc(topics).T.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO1kWogMg65w",
    "outputId": "e415cad2-8f1f-45ae-87ea-3d4564025436",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(gensim.matutils.corpus2csc(topics).toarray())\n",
    "topics_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FitLDAFuncition_V0_4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}